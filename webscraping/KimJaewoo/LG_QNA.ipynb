{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install requests beautifulsoup4 pandas lxml",
   "id": "492bc109259c900b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LG전자 지원 페이지 크롤링 코드\n",
    "# PyCharm 주피터 노트북 환경\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re\n",
    "\n",
    "class LGSupportCrawler:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.lge.co.kr\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"페이지 내용을 가져오는 함수 (디버깅 정보 포함)\"\"\"\n",
    "        try:\n",
    "            print(f\"페이지 요청 중: {url}\")\n",
    "            response = self.session.get(url, timeout=15)\n",
    "\n",
    "            print(f\"응답 상태 코드: {response.status_code}\")\n",
    "            print(f\"응답 헤더 Content-Type: {response.headers.get('Content-Type', 'Unknown')}\")\n",
    "            print(f\"응답 내용 길이: {len(response.text)} 문자\")\n",
    "\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # 응답 내용의 일부를 확인\n",
    "            if len(response.text) > 0:\n",
    "                print(\"응답 내용 미리보기:\")\n",
    "                print(response.text[:500] + \"...\" if len(response.text) > 500 else response.text)\n",
    "                print(\"=\" * 50)\n",
    "\n",
    "            return response.text\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"페이지 요청 실패: {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_support_page(self, html_content):\n",
    "        \"\"\"지원 페이지를 파싱하는 함수 (디버깅 정보 포함)\"\"\"\n",
    "        if not html_content:\n",
    "            print(\"HTML 내용이 비어있습니다.\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        support_items = []\n",
    "\n",
    "        # 페이지 구조 분석을 위한 디버깅 정보\n",
    "        print(\"=== 페이지 구조 분석 ===\")\n",
    "        print(f\"페이지 제목: {soup.title.get_text() if soup.title else '제목 없음'}\")\n",
    "\n",
    "        # 더 광범위한 선택자로 시도\n",
    "        selectors = [\n",
    "            'div[class*=\"list\"]',\n",
    "            'ul[class*=\"list\"]',\n",
    "            'div[class*=\"item\"]',\n",
    "            'li[class*=\"item\"]',\n",
    "            'div[class*=\"solution\"]',\n",
    "            'div[class*=\"support\"]',\n",
    "            'div[class*=\"card\"]',\n",
    "            'article',\n",
    "            '.list-item',\n",
    "            '.solution-item',\n",
    "            '.support-item'\n",
    "        ]\n",
    "\n",
    "        items_found = []\n",
    "        for selector in selectors:\n",
    "            elements = soup.select(selector)\n",
    "            if elements:\n",
    "                print(f\"선택자 '{selector}'로 {len(elements)}개 요소 발견\")\n",
    "                items_found.extend(elements)\n",
    "\n",
    "        # 중복 제거\n",
    "        unique_items = list(set(items_found))\n",
    "        print(f\"중복 제거 후 총 {len(unique_items)}개 항목\")\n",
    "\n",
    "        # 항목이 없으면 전체 구조 출력\n",
    "        if not unique_items:\n",
    "            print(\"항목을 찾을 수 없습니다. 페이지 구조를 분석합니다...\")\n",
    "            # 주요 div 요소들 확인\n",
    "            divs = soup.find_all('div', limit=20)\n",
    "            for i, div in enumerate(divs):\n",
    "                classes = div.get('class', [])\n",
    "                if classes:\n",
    "                    print(f\"div {i+1}: class={classes}\")\n",
    "\n",
    "            # 리스트 요소들 확인\n",
    "            lists = soup.find_all(['ul', 'ol'], limit=10)\n",
    "            for i, lst in enumerate(lists):\n",
    "                classes = lst.get('class', [])\n",
    "                if classes:\n",
    "                    print(f\"list {i+1}: class={classes}\")\n",
    "\n",
    "        # 각 항목 파싱\n",
    "        for item in unique_items:\n",
    "            try:\n",
    "                # 다양한 방법으로 제목 추출\n",
    "                title = None\n",
    "                title_selectors = [\n",
    "                    'h1, h2, h3, h4, h5, h6',\n",
    "                    'a[href*=\"solution\"]',\n",
    "                    'a[href*=\"support\"]',\n",
    "                    '.title',\n",
    "                    '.subject',\n",
    "                    '.name',\n",
    "                    'strong',\n",
    "                    'span[class*=\"title\"]'\n",
    "                ]\n",
    "\n",
    "                for selector in title_selectors:\n",
    "                    title_elem = item.select_one(selector)\n",
    "                    if title_elem:\n",
    "                        title = title_elem.get_text(strip=True)\n",
    "                        if title:\n",
    "                            break\n",
    "\n",
    "                if not title:\n",
    "                    # 텍스트 내용에서 제목 추출 시도\n",
    "                    text = item.get_text(strip=True)\n",
    "                    if text and len(text) > 10:\n",
    "                        title = text[:100] + \"...\" if len(text) > 100 else text\n",
    "\n",
    "                # 링크 추출\n",
    "                link = \"\"\n",
    "                link_elem = item.find('a', href=True)\n",
    "                if link_elem:\n",
    "                    href = link_elem['href']\n",
    "                    if href.startswith('http'):\n",
    "                        link = href\n",
    "                    else:\n",
    "                        link = urljoin(self.base_url, href)\n",
    "\n",
    "                # 설명 추출\n",
    "                description = \"\"\n",
    "                desc_selectors = [\n",
    "                    'p',\n",
    "                    '.desc',\n",
    "                    '.description',\n",
    "                    '.content',\n",
    "                    '.summary',\n",
    "                    'span[class*=\"desc\"]'\n",
    "                ]\n",
    "\n",
    "                for selector in desc_selectors:\n",
    "                    desc_elem = item.select_one(selector)\n",
    "                    if desc_elem:\n",
    "                        description = desc_elem.get_text(strip=True)\n",
    "                        if description:\n",
    "                            break\n",
    "\n",
    "                # 날짜 추출\n",
    "                date = \"\"\n",
    "                date_selectors = [\n",
    "                    'time',\n",
    "                    '.date',\n",
    "                    '.time',\n",
    "                    'span[class*=\"date\"]',\n",
    "                    'span[class*=\"time\"]'\n",
    "                ]\n",
    "\n",
    "                for selector in date_selectors:\n",
    "                    date_elem = item.select_one(selector)\n",
    "                    if date_elem:\n",
    "                        date = date_elem.get_text(strip=True)\n",
    "                        if date:\n",
    "                            break\n",
    "\n",
    "                # 유효한 항목인지 확인\n",
    "                if title and len(title.strip()) > 0:\n",
    "                    support_items.append({\n",
    "                        'title': title,\n",
    "                        'description': description,\n",
    "                        'link': link,\n",
    "                        'date': date\n",
    "                    })\n",
    "                    print(f\"항목 추가: {title[:50]}...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"항목 파싱 중 오류: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"총 {len(support_items)}개 항목 파싱 완료\")\n",
    "        return support_items\n",
    "\n",
    "    def crawl_support_solutions(self, url):\n",
    "        \"\"\"지원 솔루션 페이지를 크롤링하는 메인 함수\"\"\"\n",
    "        print(f\"크롤링 시작: {url}\")\n",
    "\n",
    "        html_content = self.get_page_content(url)\n",
    "        if not html_content:\n",
    "            return []\n",
    "\n",
    "        support_items = self.parse_support_page(html_content)\n",
    "\n",
    "        # 페이지네이션 처리 (필요시)\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        pagination = soup.find(['div', 'nav'], class_=re.compile(r'pag|page'))\n",
    "\n",
    "        if pagination:\n",
    "            page_links = pagination.find_all('a', href=True)\n",
    "            for page_link in page_links:\n",
    "                if re.search(r'page=\\d+', page_link['href']):\n",
    "                    page_url = urljoin(self.base_url, page_link['href'])\n",
    "                    print(f\"다음 페이지 크롤링: {page_url}\")\n",
    "\n",
    "                    time.sleep(1)  # 서버 부하 방지\n",
    "                    page_content = self.get_page_content(page_url)\n",
    "                    if page_content:\n",
    "                        page_items = self.parse_support_page(page_content)\n",
    "                        support_items.extend(page_items)\n",
    "\n",
    "        return support_items\n",
    "\n",
    "    def save_to_csv(self, data, filename='lg_support_data.csv'):\n",
    "        \"\"\"데이터를 CSV 파일로 저장\"\"\"\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"데이터가 {filename}에 저장되었습니다.\")\n",
    "        else:\n",
    "            print(\"저장할 데이터가 없습니다.\")\n",
    "\n",
    "    def save_to_json(self, data, filename='lg_support_data.json'):\n",
    "        \"\"\"데이터를 JSON 파일로 저장\"\"\"\n",
    "        if data:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"데이터가 {filename}에 저장되었습니다.\")\n",
    "        else:\n",
    "            print(\"저장할 데이터가 없습니다.\")\n",
    "\n",
    "# 사용 예시 (디버깅 모드)\n",
    "def main():\n",
    "    # 크롤러 인스턴스 생성\n",
    "    crawler = LGSupportCrawler()\n",
    "\n",
    "    # 대상 URL (드럼세탁기 지원 페이지)\n",
    "    target_url = \"https://www.lge.co.kr/support/solutions?category=CT50019274&subCategory=CT50019309&categoryNm=%EC%84%B8%ED%83%81%EA%B8%B0/%EA%B1%B4%EC%A1%B0%EA%B8%B0/%EC%9D%98%EB%A5%98%EA%B4%80%EB%A6%AC%EA%B8%B0&subCategoryNm=%EB%93%9C%EB%9F%BC%EC%84%B8%ED%83%81%EA%B8%B0&mktModelCd=F0Q8CSVKW9.AKOR&sort=update&page=1&seq=0&sympCodeThree=1RRRTRT\"\n",
    "\n",
    "    # 단순한 URL로도 테스트\n",
    "    simple_url = \"https://www.lge.co.kr/support/solutions\"\n",
    "\n",
    "    print(\"=== 원본 URL 테스트 ===\")\n",
    "    support_data = crawler.crawl_support_solutions(target_url)\n",
    "\n",
    "    # 원본 URL이 실패하면 단순 URL로 시도\n",
    "    if not support_data:\n",
    "        print(\"\\n=== 단순 URL로 재시도 ===\")\n",
    "        support_data = crawler.crawl_support_solutions(simple_url)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"\\n총 {len(support_data)}개의 지원 항목을 수집했습니다.\")\n",
    "\n",
    "    # 처음 5개 항목 미리보기\n",
    "    if support_data:\n",
    "        print(\"\\n=== 수집된 데이터 미리보기 ===\")\n",
    "        for i, item in enumerate(support_data[:5], 1):\n",
    "            print(f\"{i}. {item['title']}\")\n",
    "            if item['description']:\n",
    "                print(f\"   설명: {item['description'][:100]}...\")\n",
    "            print(f\"   링크: {item['link']}\")\n",
    "            print(f\"   날짜: {item['date']}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    # 데이터 저장\n",
    "    if support_data:\n",
    "        crawler.save_to_csv(support_data)\n",
    "        crawler.save_to_json(support_data)\n",
    "    else:\n",
    "        print(\"\\n=== 문제 해결 방법 ===\")\n",
    "        print(\"1. 네트워크 연결 확인\")\n",
    "        print(\"2. URL이 올바른지 확인\")\n",
    "        print(\"3. 웹사이트가 JavaScript로 동적 로딩하는지 확인\")\n",
    "        print(\"4. 웹사이트의 robots.txt 확인\")\n",
    "        print(\"5. User-Agent 헤더 변경 시도\")\n",
    "\n",
    "    return support_data\n",
    "\n",
    "# 추가 디버깅 함수\n",
    "def debug_page_structure(url):\n",
    "    \"\"\"페이지 구조를 자세히 분석하는 디버깅 함수\"\"\"\n",
    "    crawler = LGSupportCrawler()\n",
    "    html_content = crawler.get_page_content(url)\n",
    "\n",
    "    if html_content:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        print(\"=== 페이지 구조 상세 분석 ===\")\n",
    "        print(f\"전체 HTML 길이: {len(html_content)}\")\n",
    "\n",
    "        # 주요 태그들 개수 확인\n",
    "        tags_to_check = ['div', 'ul', 'li', 'article', 'section', 'h1', 'h2', 'h3', 'a']\n",
    "        for tag in tags_to_check:\n",
    "            count = len(soup.find_all(tag))\n",
    "            print(f\"{tag} 태그 개수: {count}\")\n",
    "\n",
    "        # JavaScript 확인\n",
    "        scripts = soup.find_all('script')\n",
    "        print(f\"JavaScript 태그 개수: {len(scripts)}\")\n",
    "\n",
    "        # 메타 태그 확인\n",
    "        meta_tags = soup.find_all('meta')\n",
    "        for meta in meta_tags:\n",
    "            if meta.get('name') or meta.get('property'):\n",
    "                print(f\"Meta: {meta.get('name', meta.get('property', 'unknown'))}\")\n",
    "\n",
    "        # CSS 클래스 분석\n",
    "        all_elements = soup.find_all(True)\n",
    "        classes = set()\n",
    "        for element in all_elements:\n",
    "            if element.get('class'):\n",
    "                classes.update(element.get('class'))\n",
    "\n",
    "        print(f\"사용된 CSS 클래스 개수: {len(classes)}\")\n",
    "        relevant_classes = [cls for cls in classes if any(keyword in cls.lower() for keyword in ['list', 'item', 'solution', 'support', 'card'])]\n",
    "        print(f\"관련 CSS 클래스: {relevant_classes[:10]}\")\n",
    "\n",
    "    return html_content\n",
    "\n",
    "# 주피터 노트북에서 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 필요한 라이브러리 설치 (주피터 노트북에서 실행)\n",
    "    # !pip install requests beautifulsoup4 pandas lxml\n",
    "\n",
    "    # 크롤링 실행\n",
    "    data = main()\n",
    "\n",
    "    # 데이터 분석 예시\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"\\n=== 데이터 분석 결과 ===\")\n",
    "        print(f\"총 항목 수: {len(df)}\")\n",
    "        print(f\"링크가 있는 항목: {len(df[df['link'] != ''])}\")\n",
    "        print(f\"설명이 있는 항목: {len(df[df['description'] != ''])}\")\n",
    "\n",
    "        # 제목 길이 분석\n",
    "        if len(df) > 0:\n",
    "            df['title_length'] = df['title'].str.len()\n",
    "            print(f\"평균 제목 길이: {df['title_length'].mean():.1f}자\")\n",
    "            print(f\"가장 긴 제목: {df['title_length'].max()}자\")\n",
    "            print(f\"가장 짧은 제목: {df['title_length'].min()}자\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
